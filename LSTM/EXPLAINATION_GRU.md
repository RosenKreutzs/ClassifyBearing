# GRU门控循环单元神经网络)

## 1. GRU与LSTM的区别

LSTM具有三个门（输入门、遗忘门、输出门），GRU具有两个门（更新门和重置门），相比LSTM，GRU的结构更为简单。

## 2. GRU的应用场景和优缺点

### 应用场景

1. **时序数据预测**：GRU网络由于其记忆能力，特别适用于处理时序数据。例如，在风力发电预测控制、电力系统的负荷预测以及工业过程的时序控制等领域，GRU网络都取得了良好的应用效果。
2. **自然语言处理**：GRU网络在处理自然语言处理任务，如自动语音转写、文本分类、机器翻译等方面也有广泛应用。通过训练大量的文本数据，GRU网络可以学习到语言的规律和模式，从而实现对文本的智能理解和分析。
3. **控制领域**：GRU网络在控制领域有着广泛的应用，如解决非线性控制问题。通过与神经网络控制技术相结合，GRU网络能够实现对非线性系统的精确建模和控制。

### 优点

1. **记忆能力**：GRU网络具有记忆能力，能够有效地处理序列数据，对于时序数据的预测和分析非常有效。
2. **强大的非线性映射能力**：神经网络控制技术结合GRU网络，具有强大的非线性映射能力，可以对复杂的动态系统进行建模和学习。
3. **自适应、自组织能力**：GRU网络能够针对系统的不确定性和变化进行实时调整，提高控制性能，具有较强的自适应和自组织能力。
4. **解决复杂问题**：GRU网络可以有效地解决复杂的问题，如图像识别、语音识别、自然语言处理等。

### 缺点

1. **训练成本高**：神经网络的训练需要大量的计算资源和时间成本，尤其是对于大规模的数据集和复杂的模型来说，训练成本更加高昂。
2. **模型复杂难于解释**：GRU网络由多个神经元和层数组成，使得模型变得异常复杂，难以解释和理解。
3. **对硬件要求高**：神经网络的训练和推理需要高性能的算子和硬件支持，如GPU等，这对硬件设备的要求较高。
4. **容易过拟合**：由于神经网络具有强大的建模能力，它可能会过于复杂，导致过拟合现象，使得模型在训练数据上的性能很好，但在测试数据上的性能较差。

## 3. GRU的运算逻辑(以时间点为单个对象，并只看一层)

### 前向传播算法

#### 更新门的算法

$$
Z_t = σ_z(W_z \cdot [h_{t-1},x_t] + b_z)
$$

- $Z_t$是更新门输出值；
- $σ_z$是更新门的激活函数；
- $W_z$是更新门的权重矩阵；
- $b_z$是更新门的偏移值向量；

#### 重置门的算法

$$
R_t = σ_z(W_r \cdot [h_{t-1},x_t] + b_r)
$$

- $R_t$是重置门输出值向量；
- $σ_r$是重置门的激活函数；
- $W_r$是重置门的权重矩阵；
- $b_r$是重置门的偏移值向量；

#### 候选隐藏状态值的算法

$$
\tilde{h}_t = tanh(W_{\tilde{h}} \cdot [ R_t \cdot h_{t-1},x_t] + b_{\tilde{h}})
$$

- $\tilde{h}_t$是该时间点的 候选隐藏状态值向量；
- $W_{\tilde{h}}$是候选隐藏状态值的算法的权重矩阵；
- $b_{\tilde{h}}$是 候选隐藏状态值的算法的偏移值向量；

#### 隐藏状态值的算法

$$
h_t = (1-Z_t) \cdot h_{t-1} + Z_t \cdot \tilde{h}_t
$$

### 反向传播算法

对于单个循环核的单个时间点而言，偏移值一个向量，包含多个标量；与单个循环核的权重一样都是向量；

对于对于单个循环核而言，偏移值与权重都是矩阵；两矩阵组合为一个大的参数矩阵集合θ;(θ={ $W_{xh}$ , $W_{hh}$ , $W_{hy}$, $b_h$ , $b_y$ })

GRU的反向传播直接更新每一层的参数矩阵集合θ;(参数矩阵集合θ的各个矩阵对象的反向传播算法是一样)

下面算法用θ代指各个单独的矩阵对象；

#### 损失项

- 输出值的误差项：
- 隐藏状态值的误差项：$δ_{y_t}=\frac{∂L}{∂y_t} \cdot \frac{∂y_t}{∂h_t}$;

  - 当时间点为T时：$δ_{h_T}=δ_{y_T}$;
  - 当时间点小于T时：$δ_{h_t}=δ_{y_t} + δ_{h_{t+1}} \cdot \frac{∂h_{t+1}}{∂h_t}$;
- 门控单元和候选集的误差项:

  - $δ_{z_t} = δ_{h_t} ⊙ \tilde{h}_t ⊙ (1-z_t)$;
  - $δ_{r_t} = δ_{h_t} ⊙ z_t ⊙ (1- \tilde{h}_t^2) ⊙ δ_{\tilde{h}_t} \cdot W_h^T$;
  - $δ_{\tilde{h}_t} = δ_{h_t} ⊙ δ_{\tilde{h}_t} ⊙ (1-z_t)$;
#### 梯度
- $\frac{∂L}{∂W_z} = \sum_{t}^T δ_{z_t} ⊙ x_t^T$;
- $\frac{∂L}{∂b_z} = \sum_{t}^T δ_{z_t}$
- $\frac{∂L}{∂W_r} = \sum_{t}^T δ_{r_t} ⊙ x_t^T$;
- $\frac{∂L}{∂b_r} = \sum_{t}^T δ_{r_t}$
- $\frac{∂L}{∂W_h} = \sum_{t}^T δ_{h_t} ⊙ x_t^T$;
- $\frac{∂L}{∂b_h} = \sum_{t}^T δ_{h_t}$
#### 更新参数（用θ代指所有参数）

$$
θ_{new}= θ_{old} - η \cdot \frac{∂L}{∂θ_{old}}
$$
