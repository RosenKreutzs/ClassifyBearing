# LSTM(长短期记忆神经网络)

## 1. LSTM与RNN的区别

LSTM是RNN的一种变体，其与RNN最大的区别是在在前向传播中添加细胞状态值c，用于调整隐藏状态值h；
这样就可以避免由于时间步太长，而损失过前时间步信息的问题;(细胞状态值c也是和时间点一一对应的)

## 2. LSTM的应用场景和优缺点

### **LSTM的应用场景**

LSTM（长短期记忆网络）因其在处理序列数据方面的优势，被广泛应用于多个领域。以下是一些主要的应用场景：

1. **自然语言处理（NLP）**：LSTM在文本分类、情感分析、机器翻译、语言模型和语音识别等领域有广泛应用。企业可以利用LSTM来分析客户反馈，自动翻译内容，或者开发智能聊天机器人。
2. **时间序列预测**：LSTM能够处理和预测金融市场、股票价格、气象预报、能源消耗等时间序列数据。
3. **语言模型**：LSTM可以用于生成文本，如自动生成文章、对话、歌词等。通过学习大量的文本数据，LSTM可以学习到语言的概率分布，从而能够生成新的、具有连贯性的文本。
4. **视频分析**：LSTM可以用于视频分析，如动作识别、行为分析等。通过学习视频序列中的时间依赖关系，LSTM可以对不同的动作进行分类和识别。

### **LSTM的优点**

1. **处理长期依赖性**：LSTM通过引入记忆单元和门控机制来解决传统RNN中的梯度消失和梯度爆炸问题，使其在处理长序列和长期依赖性任务时表现更出色。
2. **高度可控的记忆单元**：LSTM中的记忆单元可以通过遗忘门、输入门和输出门等门控机制对输入进行选择性记忆或遗忘，这种机制使得LSTM能够有效地处理不同时间步长上的信息，灵活地控制记忆单元的写入和读取。
3. **多层结构**：LSTM可以堆叠多个LSTM层，每一层都可以学习不同抽象层次的特征。通过增加网络深度，LSTM可以提高模型的表示能力和学习能力，更好地捕捉数据中的复杂模式。
4. **抗噪性强**：由于LSTM的门控机制，它对于输入中的噪声和异常值具有一定的鲁棒性，能够选择性地忽略或减少对噪声的响应，从而提高模型的鲁棒性和泛化能力。
5. **并行计算性能好**：LSTM中的门控机制使得各个时间步之间的计算可以并行进行，这样可以提高训练和推理的效率。

### **LSTM的缺点**

1. **复杂性**：LSTM比传统的RNN更复杂，其复杂性和涉及的参数数量使得LSTM更容易出现过度拟合。
2. **计算密集型**：与CNN或RNN相比，LSTM需要更多的计算资源。
3. **训练时间长**：由于其复杂性和循环计算的性质，LSTM可能需要很长时间来训练，尤其是在较大的数据集上。
4. **梯度消失和爆炸**：虽然LSTM的设计目的是在一定程度上缓解梯度消失问题，但它们仍然会受到影响，尤其是对于很长的序列。同样，它们也可能遭受梯度爆炸问题，即梯度变得太大并破坏学习过程的稳定性。
5. **不适用于空间数据**：LSTM非常适合序列数据，但不适用于空间数据。具有卷积和池化操作的CNN更适合处理图像等空间数据。

## 4、3. LSTM的相关代码

[LSTM的相关代码](./TRY.ipynb)

## 4. LSTM的运算逻辑

### LSTM的前向传播算法：( 输入门 to 遗忘门 to 输出门)

#### 输出门算法

##### 输出值的计算方法：

$$
O_{i,j,t} =σ_{i,o}(W_{i,j,t,o} \cdot [h_{i,j,t-1},x_{i,t}] + b_{i,j,t,o})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应隐藏状态值$h_{i,j,t-1}$，计算该时间点的输出值$O_{i,j,t}$；
- $σ_{i,o}()$是第i层的输出门的激活函数；
- $b_{i,j,t,o}$是第i层第j个神经元的输出门的偏置项；
- $W_{i,j,t,o}$是第i层第j个神经元的输出门的权重向量；
- 在第i层第j个神经元上，对应一个输入对象算出所有的时间点的输出值$O_{i,j,t}$，并将这些输出值组成一个向量，作为一个循环核输出的输出向量；
- 一层循环层的多个循环核输出的对应输出值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

##### 隐藏状态值的计算方法：

$$
h_{i,j,t}= O_{i,j,t} \cdot tanh(c_{i,j,t})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应细胞状态值$c_{i,j,t}$，计算的是该时间点对应隐藏状态值$h_{i,j,t}$；
- $O_{i,j,t}$是该时间点的输出值；
- tanh 是双曲正切函数；

#### 遗忘门算法

##### 更新细胞状态值的计算方法

$$
c_{i,j,t}= f_{i,j,t} \cdot c_{i,j,t-1} +i_{i,j,t} \cdot \tilde{c}_{i,j,t}
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），计算的是该时间点对应细胞状态值$c_{i,j,t}$；
- $c_{i,j,t-1}$是上一个时间点的细胞状态值；
- $\tilde{c}_{i,j,t}$是这个时间点的细胞过度状态值；
- $i_{i,j,t}$是这个时间点的加权输入值；
- $f_{i,j,t}$是这个时间点的遗忘函数值；

##### 遗忘函数值的计算方法

$$
f_{i,j,t} =σ_{i,F}(W_{i,j,t,F} \cdot [h_{i,j,t-1},x_{i,t}] + b_{i,j,t,F})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应隐藏状态值$h_{i,j,t-1}$，计算该时间点的遗忘函数值$f_{i,j,t}$；
- $σ_{i,F}()$是第i层的遗忘门的激活函数；
- $b_{i,j,t,F}$是第i层第j个神经元的遗忘门的偏置项；
- $W_{i,j,t,F}$是第i层第j个神经元的遗忘门的权重向量；
- 在第i层第j个神经元上，对应一个输入对象算出所有的时间点的遗忘函数值$O_{i,j,t}$，并将这些值组成一个向量，作为一个循环核输出的遗忘函数值向量；
- 一层循环层的多个循环核输出的对应遗忘函数值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

#### 输入门算法

##### 加权输入值的计算方法

$$
i_{i,j,t} =σ_{i,i}(W_{i,j,t,i} \cdot [h_{i,j,t-1},x_{i,t}] + b_{i,j,t,i})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应隐藏状态值$h_{i,j,t-1}$，计算该时间点的加权输入值$i_{i,j,t}$；
- $σ_{i,i}()$是第i层的输入门的激活函数；
- $b_{i,j,t,i}$是第i层第j个神经元的输入门的加权输入函数的偏置项；
- $W_{i,j,t,i}$是第i层第j个神经元的输入门的加权输入函数的权重向量；
- 在第i层第j个神经元上，对应一个输入对象算出所有的时间点的加权输入值$i_{i,j,t}$，并将这些值组成一个向量，作为一个循环核输出的加权输入值向量；
- 一层循环层的多个循环核输出的对应加权输入值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

##### 细胞过度状态值的计算方法

$$
\tilde{c}_{i,j,t} =tanh(W_{i,j,t,c} \cdot [h_{i,j,t-1},x_{i,t}] + b_{i,j,t,c})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应隐藏状态值$h_{i,j,t-1}$，计算该时间点的细胞过度状态值$\tilde{c}_{i,j,t}$；
- $b_{i,j,t,i}$是第i层第j个神经元的输入门的细胞过度状态值对应的偏置项；
- $W_{i,j,t,i}$是第i层第j个神经元的输入门的细胞过度状态值对应的权重向量；
- 在第i层第j个神经元上，对应一个输入对象算出所有的时间点的细胞过度状态值$\tilde{c}_{i,j,t}$，并将这些值组成一个向量，作为一个循环核输出的细胞过度状态值向量；
- 一层循环层的多个循环核输出的对应细胞过度状态值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

### LSTM的反向传播算法

对于单个循环核的单个时间点而言，偏移值一个向量，包含多个标量；与单个循环核的权重一样都是向量；

对于对于单个循环核而言，偏移值与权重都是矩阵；两矩阵组合为一个大的参数矩阵集合θ;(θ={ $W_{xh}$ , $W_{hh}$ , $W_{hy}$, $b_h$ , $b_y$ })

LSTM的反向传播直接更新每一层的参数矩阵集合θ;(参数矩阵集合θ的各个矩阵对象的反向传播算法是一样)

下面算法用θ代指各个单独的矩阵对象；

#### #### 对应整个LSTM莫的梯度计算（按时间点为对象，将不同神经元的相同参数合成对应的多个参数矩阵）

##### t时间点的各个损失项的计算

- 隐藏状态值的损失项：$δ_{h_t} =\frac{∂L}{∂h_t}$;
- 输出值的损失项：$δ_{O_t} =δ_{h_t} \cdot tanh(c_t) \cdot O_t \cdot (1-O_t)$;
- 细胞状态值的损失项：$δ_{c_t} =δ_{h_t} \cdot O_t \cdot (1-tanh(c_t)^2) + δ_{c_{t+1}} \cdot f_{t+1} $;
- 加权输入值的损失项：$δ_{i_t} =δ_{c_t} \cdot i_t \cdot \tilde{c}_t \cdot (1-i_t) $;
- 细胞过度状态值的损失项：$δ_{\tilde{c}_t} =δ_{c_t} \cdot i_t \cdot (1-\tilde{c}_t^2) $;
- 遗忘函数值的损失项：$δ_{f_t} = δ_{c_t} \cdot c_{t-1} \cdot f_t \cdot (1-f_t)$

##### 各个梯度的计算(全是向量)

- 输出值的权重矩阵的损失项：$\frac{∂L}{∂W_o} = \sum_{t=1}^T δ_{O_t} \cdot [h_{t-1},x_t]^T$
- 输出值的偏移值的损失项：$\frac{∂L}{∂b_o} = \sum_{t=1}^T δ_{O_t} $
- 加权输入值的权重矩阵的损失项：$\frac{∂L}{∂W_i} = \sum_{t=1}^T δ_{i_t} \cdot [h_{t-1},x_t]^T$
- 加权输入值的偏移值的损失项：$\frac{∂L}{∂b_i} = \sum_{t=1}^T δ_{i_t} $
- 遗忘函数值的权重矩阵的损失项：$\frac{∂L}{∂W_f} = \sum_{t=1}^T δ_{f_t} \cdot [h_{t-1},x_t]^T$
- 遗忘函数值的偏移值的损失项：$\frac{∂L}{∂b_f} = \sum_{t=1}^T δ_{f_t} $
- 细胞状态值的权重矩阵的损失项：$\frac{∂L}{∂W_c} = \sum_{t=1}^T δ_{c_t} \cdot [h_{t-1},x_t]^T$
- 细胞状态值的偏移值的损失项：$\frac{∂L}{∂b_c} = \sum_{t=1}^T δ_{c_t} $

#### 隐藏状态的梯度传播(全是矩阵)

$$
δ_{h_{t-1}} = δ_{o_t} \cdot W_o + δ_{i_t} \cdot W_i + δ_{f_t} \cdot W_f + δ_{\tilde{c}_t} \cdot W_c
$$

#### 参数更新

$$
θ_{new}= θ_{old} - η \cdot \frac{∂L}{∂θ_{old}}
$$
