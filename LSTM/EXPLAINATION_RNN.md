# RNN(循环神经网络)

## 1. RNN的本质

一个有13个卷积层和3个全连接层且滑动窗口大小都为(3,3)的卷积神经网络，
使用3个3x3的卷积核，这样做可以在保证感受野大小不变的同时，减少参数数量，提高计算效率。

## 2. RNN的应用场景和优缺点

### RNN的应用场景

1. **自然语言处理（NLP）**

   - **语言建模**：RNN可以根据前文预测下一个单词或字符，用于自动文本生成、拼写纠错等任务。
   - **机器翻译**：RNN可以将输入语言的序列转换为输出语言的序列，实现自动翻译。
   - **文本分类**：RNN可以对文本进行情感分析、垃圾邮件过滤等分类任务。
   - **命名实体识别**：RNN可以识别文本中的人名、地名、组织名等实体。
2. **语音识别**

   - **语音识别**：RNN可以将声音信号转换为文字，用于语音助手、语音命令识别等场景。
   - **语音合成**：RNN可以将文字转换为声音信号，实现自然语音合成。
3. **时间序列预测**

   - **股票预测**：RNN可以根据历史股票数据预测未来股价走势。
   - **天气预测**：RNN可以利用历史气象数据预测未来的天气情况。
   - **负载预测**：RNN可以根据历史负载数据预测未来的系统负载。
4. **图像/视频描述或字幕生成**

   - RNN模型可与CNN结合使用，以生成图像和视频中找到的元素的描述。

### RNN的优缺点

#### 优点

1. **处理序列数据**：RNN是一种适用于处理序列数据的神经网络，能够有效地捕捉序列中的时序信息。
2. **共享权重**：RNN在每个时间步都使用相同的参数，可以有效地共享权重，减少模型的复杂度和训练的参数数量。
3. **上下文依赖建模**：RNN能够记忆之前的信息，并在后续时间步中利用该信息进行预测或决策，对于处理依赖于上下文的任务非常有用。
4. **处理任意长度的输入**：RNN可以处理任意长度的输入序列，并且模型形状不随输入长度增加而改变。

#### 缺点

1. **梯度消失/爆炸**：RNN在反向传播时，由于参数共享和多次连乘的特性，容易出现梯度消失或梯度爆炸的问题，导致模型难以训练或无法收敛。
2. **长期依赖问题**：由于梯度消失的原因，RNN在处理长序列时难以捕捉到长期依赖关系，只能有效利用较短的上下文信息。
3. **计算效率较低**：RNN的计算过程是基于时间步展开的，每个时间步都需要依次计算，造成计算效率较低，尤其是处理较长序列时。
4. **无法考虑未来输入**：RNN主要基于历史信息进行预测，无法考虑当前状态的任何未来输入。

## 3. RNN的运算逻辑

### 前向传播算法：(RNN除了输出层输出目标标签外，其他层输出的都是对应层隐藏状态值序列)

#### 前向传播算法中计算隐藏状态值的算法：

$$
h_{i,j,t}=f_i(w_{i,j,t,xh} \cdot x_{i,t} + w_{i,j,t,hh} \cdot h_{i,j,t-1}+b_{i,j,t,h})
$$

- 此方程是在单个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据）计算对应时间点隐藏状态值$h_{i,j,t}$的公式；（$x_{i,t}，h_{i,j,t}，h_{i,j,t-1}$都是标量）
- $h_{i,j,t-1}$为在第i层第j个神经元上，上一个时间点的隐藏状态值；
- $x_{i,t}$为在第i层的第t个时间点的单个元素；
- $f_i()$为第i层的隐藏状态值的激活函数；
- $w_{i,j,t,hh}$为第i层第j个神经元第t-1个时间点的隐藏状态值计算第i层第j个神经元第t个时间点的隐藏状态值的权重向量；
- $w_{i,j,t,xh}$为第i层第j个神经元第t个时间点的输入元素计算第i层第j个神经元第t个时间点的隐藏状态值的权重向量；
- $b_{i,j,t,h}$为第i层第j个神经元第t个时间点计算第i层第j个神经元第t个时间点的隐藏状态值的偏置标量项；
- 在单个神经元上，对于一个输入对象算出所有的时间点的隐藏状态值，并将这些隐藏状态值组成一个向量，作为一个循环核输出的隐藏状态值向量；
- 一层循环层的多个循环核输出的对应隐藏状态值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

#### 前向传播算法中输出值的算法：

$$
y_{i,j,t}=g_i(w_{i,j,t,hy} \cdot h_{i,j,t} + b_{i,j,t,y})
$$

- 此方程是在第i层第j个神经元上，为一个输入对象的单个元素$x_{i,t}$（单个时间点t的数据），使用的是该时间点对应隐藏状态值$h_{i,j,t}$，计算该时间点的输出值$y_{i,j,t}$；
- $(h_{i,j,t}$为在第i层第j个神经元上，该时间点的隐藏状态值；
- $g_i()$为第i层的输出值的激活函数；
- $w_{i,j,t,hy}$第i层第j个神经元第t个时间点的隐藏状态值计算第i层第j个神经元第t个时间点的输出值的权重向量；
- $b_{i,j,t,y}$为第i层第j个神经元第t个时间点计算第i层第j个神经元第t个时间点的输出值的偏置标量项；
- 在第i层第j个神经元上，对应一个输入对象算出所有的时间点的输出值$y_{i,j,t}$，并将这些输出值组成一个向量，作为一个循环核输出的输出向量；
- 一层循环层的多个循环核输出的对应输出值向量，堆叠为一个二维矩阵；(这个矩阵的每一列都对应输入对象的一个时间点，再下一层循环层计算时，就以一列作为单个元素计算)

### 反向传播算法：

对于单个循环核的单个时间点而言，偏移值一个向量，包含多个标量；与单个循环核的权重一样都是向量；

对于对于单个循环核而言，偏移值与权重都是矩阵；两矩阵组合为一个大的参数矩阵集合θ;(θ={ $W_{xh}$ , $W_{hh}$ , $W_{hy}$, $b_h$ , $b_y$ })

RNN的反向传播直接更新每一层的参数矩阵集合θ;(参数矩阵集合θ的各个矩阵对象的反向传播算法是一样)

下面算法用θ代指各个单独的矩阵对象；

#### 更新一层参数矩阵θ的算法；

$$
θ_i←θ_i−η \cdot \frac{∂L}{∂θ}
$$

- $\frac{∂L}{∂θ}$是第i层的梯度矩阵；
- $θ_i$是第i层的参数矩阵；

#### 第i层的梯度的计算；

$$
\frac{∂L}{∂θ} = \sum_{t=1}^{T} [\frac{∂L_t}{∂y_t} \cdot \frac{∂y_t}{∂h_t} \cdot (\sum_{k=0}^{t-1} \frac{∂h_k}{∂θ} \cdot (\prod_{j=k+1}^{t} \frac{∂h_j}{∂h_{j-1}}) )]
$$

- 总损失项L是所有时间步损失函数的输出$L_t$的和，也是一层损失项，即$L=\sum_{t=1}^{T} L_t$;(t是时间步的索引)
  - T为整个输入对象的长度；
  - 我们需要计算的是总损失对网络参数矩阵𝜃的梯度$\frac{∂L}{∂θ}$;
  - $\frac{∂L_t}{∂y_t}$这是第t个时间步上损失$L_t$对输出值向量$y_t$的梯度。
  - $\frac{∂y_t}{∂h_t}$这是第t个时间步上输出$y_t$对隐藏状态值向量$h_t$的梯度。
- $\sum_{k=0}^{t-1} \frac{∂h_k}{∂θ} \cdot (\prod_{j=k+1}^{t} \frac{∂h_j}{∂h_{j-1}}) $这一部分表示从时间步0到t−1的所有隐藏状态对参数矩阵θ的影响，并且需要乘以这些时间步之间的隐藏状态梯度的连乘积。
  - $\frac{∂h_k}{∂θ}$是隐藏状态值向量$h_k$对参数矩阵𝜃的梯度。
  - $\prod_{j=k+1}^{t} \frac{∂h_j}{∂h_{j-1}}$是从时间步k+1到t的隐藏状态向量之间梯度的连乘积，表示梯度在这些时间步之间的传播。
- 注意：上方算法的θ代指各个单独的矩阵对象,由于各个单独的矩阵对象的反向传播算法是一样的不想重新；
- 本身的θ是等于{ $W_{xh}$ , $W_{hh}$ , $W_{hy}$, $b_h$ , $b_y$ }，本身的$\frac{∂L}{∂θ}$等于{ $\frac{∂L}{∂W_{xh}}$ , $\frac{∂L}{∂W_{hh}}$ , $\frac{∂L}{∂W_{hy}}$, $\frac{∂L}{∂b_h}$ , $\frac{∂L}{∂b_y}$ }
## 2. 循环神经网络的双向机制
一层双向RNN：包含两个单向RNN层，一个正向RNN（从左到右）和一个反向RNN（从右到左）。
这两个单向RNN层的隐藏状态在每个时间步都被合并（例如，通过相加、取平均或拼接），
以产生一个包含双向信息的隐藏状态。